# @package _global_
defaults:
  - override /datamodule: mm.yaml
  - override /model: cxr_fu_musicvit_bert.yaml
  - override /engine: cxr_vision_musicvit_bert.yaml
  - override /optimizer: adamw.yaml
  - override /scheduler: cosine_annealing.yaml
  - override /callbacks: wandb.yaml
  - override /logger: wandb.yaml
  - override /trainer: ddp.yaml
  - override /log_dir: default.yaml

project: "CXR_FU_image_text"
name: "230307_mm_base_report_unfreeze_bert" #"230205_singleGPU_freeze_bert_false"
seed: 100
image_size: 512

# engine
engine:
  num_class: 2
  disease_lambda: 0.1   # 0.1
  matching_lambda: 0.01  # 0.01
  criterion: ['CE','BCE'] # change, # disease

# datamodule
datamodule:
  data_dir: '/mnt/nas252/Kyungjin.Cho/_jykim/MuSiC-ViT-main/MuSiC-ViT-main/json/'
  batch_size: 9
  num_workers: 4
  mean: 0.2
  std: 0.4

  dataset_config:
    max_words: 128 # M3AE max_text_len = 40, #MGCA max word is 112
    pretrained_tokenizer: "emilyalsentzer/Bio_ClinicalBERT" #"emilyalsentzer/Bio_ClinicalBERT"
    d_num: 14
    ch_noch: False
    gold: False
    text_name: "_comp_fov_text_v3"
    only_base_text_name: True
    # only_fu_text_name: True
    empty_fu_sent: False
    empty_fu_sent_dummy: False
    random_erase: -1.0
    use_kd_get_item: False

  # data augmentation for train
  transforms:
    train:
      resize:
        height: ${image_size}
        width: ${image_size}
        interpolation: 1
      random_pixel_augment_music:
        p: 1
      # random_flip:
      #   p: 0.2
      # random_spatial_augment_v1:
      #   height: ${image_size}
      #   width: ${image_size}
      #   scale: 0.1
      #   rotate: 30
      #   shear_x: 15
      #   shear_y: 15
      #   translate_x: 15
      #   translate_y: 15
      #   p: 0.5
      # random_pixel_augment_v2:
      #   n: 2
      #   limit_blur: 15
      #   limit_gamma: 5
      #   limit_brightness: 0.2
      #   limit_contrast: 0.2
      #   p: 0.5
  # data augmentation for valid
    valid:
      resize:
        height: ${image_size}
        width: ${image_size}
        interpolation: 1 # 1:linear, 2:bicubic
    # data augmentation for test
    test:
      resize:
        height: ${image_size}
        width: ${image_size}
        interpolation: 1 # 1:linear, 2:bicubic
  
# model parameter
model:
  in_channels: 1
  stem_channels: 16
  cmt_channelses: [46, 92, 184, 368]
  pa_channelses: [46, 92, 184, 368]
  R: 3.6
  repeats: [2, 2, 10, 2]
  input_size: ${image_size}
  sizes: [128, 64, 32, 16]
  patch_ker: 2
  patch_str: 2
  disease_classes: 14
  num_labels: 2
  freeze_bert: False
  cxr_bert_pretrained: '/mnt/nas252/Kyungjin.Cho/_jykim/mm/FU_trainer/logs/CXR_FU_text/230225_onlybase/2023-02-25_06-36-35/checkpoints/epoch_043.ckpt'
  bert_pretrained: "emilyalsentzer/Bio_ClinicalBERT"
  output_attentions: True
  output_hidden_states: True

callbacks:
  model_checkpoint:
    monitor: "valid/change_loss"
    mode: "min" # "max" means higher metric value is better, can be also "min"
    save_top_k: 30 # save k best models (determined by above metric)
    save_last: True # additionaly always save model from last epoch
    verbose: False
    dirpath: "checkpoints/"
    filename: "epoch_{epoch:03d}"
    auto_insert_metric_name: False


trainer:
  gpus: [0,1,2]
  
  min_epochs: 1
  max_epochs: 100

  #amp_backend: "native" # auto mixed precsion
  #precision: 32

  # gradient_clip_val : 0.5 # gradient cliping, SAM optimizer don't use it
  # accumulate_grad_batches: 1 # gradient accumulation
  
  log_every_n_steps: 10
  num_sanity_val_steps: 1
  resume_from_checkpoint: null # ckpt path

scheduler:
  eta_min: 0.000001

optimizer:
  lr: 0.00002
  weight_decay: 0.01

hydra:
  run:
    dir: trainer/logs/${project}/${name}
  sweep:
    dir: trainer/logs/${project}/${name}
    subdir: ${hydra.job.num}
